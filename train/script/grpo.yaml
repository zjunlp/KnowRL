# Model parameters
model_name_or_path: "your_model_path/model_name" # Please replace with your model path
model_revision: main
torch_dtype: bfloat16
lora_r: 256
lora_alpha: 512
attn_implementation: flash_attention_2
bf16: true
tf32: true
output_dir: "output/your_output_directory_name" # Please replace with your output directory

# Dataset parameters
dataset_id_or_path: "path/to/your/dataset.json" # Please replace with your dataset path

# WandB parameters
report_to: swanlab
swanlab_project: "your_swanlab_project_name" # Please replace with your Swanlab project name
swanlab_experiment_name:: "your_swanlab_experiment_name" # Please replace with your Swanlab experiment name

# Training parameters
max_steps: 150
per_device_train_batch_size: 24
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
learning_rate: 1.0e-5
lr_scheduler_type: cosine
warmup_ratio: 0.03
seed: 42

# GRPO algorithm parameters
beta: 0.001
optim: adamw_8bit
max_prompt_length: 256
max_completion_length: 1024
num_generations: 24
use_vllm: true
vllm_gpu_memory_utilization: 0.5

# Logging parameters
logging_strategy: steps
logging_steps: 1
save_strategy: "steps"
save_steps: 50
resume_from_checkpoint: false

# Configuration file paths
grpo_config_file: "./script/grpo.yaml"


